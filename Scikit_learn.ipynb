{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0/62spK/gJPp/ouzClKH4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Himalaypatel75/My-ML-Notes/blob/main/Scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-learn (sklearn) offers a variety of powerful models for different machine learning tasks. Here are some of the top models in scikit-learn:"
      ],
      "metadata": {
        "id": "8UQWbXR5qkom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Supervised Learning**"
      ],
      "metadata": {
        "id": "zdHPVPK8ql8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `Linear Regression` ✅\n",
        "    Used for regression tasks.\n",
        "    Fits a linear model with coefficients to minimize the residual sum of squares between observed and predicted values.\n",
        "\n",
        "2. `Logistic Regression` ✅\n",
        "    Used for binary classification tasks.\n",
        "    Models the probability of a binary outcome using a logistic function.\n",
        "\n",
        "3. `Decision Trees` ✅\n",
        "    Used for both classification and regression tasks.\n",
        "    Non-parametric models that split the data into subsets based on feature values.\n",
        "\n",
        "4. `Random Forests` ✅\n",
        "    Ensemble method using multiple decision trees to improve predictive accuracy and control overfitting.\n",
        "    Can be used for both classification and regression.\n",
        "\n",
        "5. `Gradient Boosting Machines (GBM)`\n",
        "    Ensemble method that builds trees sequentially to minimize the loss function.\n",
        "    Effective for both classification and regression tasks.\n",
        "\n",
        "6. `Support Vector Machines (SVM)`\n",
        "    Can be used for both classification and regression.\n",
        "    Finds the hyperplane that maximizes the margin between classes.\n",
        "\n",
        "7. `K-Nearest Neighbors (KNN)`\n",
        "    Simple, instance-based learning method.\n",
        "    Classifies a data point based on the majority class among its k-nearest neighbors.\n",
        "\n",
        "8. `Naive Bayes`\n",
        "    Based on Bayes' theorem with the assumption of feature independence.\n",
        "    Commonly used for text classification."
      ],
      "metadata": {
        "id": "VEHbfjZXqsno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------------------------"
      ],
      "metadata": {
        "id": "pkwWLKm_rqFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unsupervised Learning**"
      ],
      "metadata": {
        "id": "AWykto-_rQEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `K-Means Clustering`\n",
        "    Partitions data into k clusters by minimizing the variance within each cluster.\n",
        "\n",
        "2. `Hierarchical Clustering`\n",
        "    Builds a tree of clusters by progressively merging or splitting clusters.\n",
        "\n",
        "3. `DBSCAN (Density-Based Spatial Clustering of Applications with Noise)`\n",
        "    Groups points that are closely packed together, marking as outliers the points that are in low-density regions.\n",
        "\n",
        "4. `Principal Component Analysis (PCA)`\n",
        "    Linear dimensionality reduction technique.\n",
        "    Transforms data to a lower-dimensional space by projecting it onto the directions of maximum variance.\n",
        "\n",
        "5. `t-SNE (t-Distributed Stochastic Neighbor Embedding)`\n",
        "    Non-linear dimensionality reduction technique.\n",
        "    Particularly well-suited for visualizing high-dimensional data."
      ],
      "metadata": {
        "id": "gPSs1ZVorR42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------"
      ],
      "metadata": {
        "id": "7HNT9cjjrsW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Selection and Evaluation**"
      ],
      "metadata": {
        "id": "NaRJWbClrnp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `Grid Search CV`\n",
        "  Exhaustive search over a specified parameter grid for a model.\n",
        "  Uses cross-validation to evaluate model performance.\n",
        "\n",
        "2. `Randomized Search CV`\n",
        "  Randomized search over specified parameter values.\n",
        "  Uses cross-validation to evaluate model performance and is generally faster than grid search.\n",
        "  \n",
        "3. `Cross-Validation`\n",
        "  Techniques like K-Fold CV are used to assess the generalizability of a model."
      ],
      "metadata": {
        "id": "RQDl_uM7ruOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------"
      ],
      "metadata": {
        "id": "lprvx8-5r2Kp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Selection**"
      ],
      "metadata": {
        "id": "BCbfeoHyr5I1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `RFE (Recursive Feature Elimination)`\n",
        "    Selects features by recursively considering smaller and smaller sets of features.\n",
        "    \n",
        "2. `SelectKBest`\n",
        "    Selects the top k features that have the highest score according to a scoring function."
      ],
      "metadata": {
        "id": "VkOC9_yir6fb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------------"
      ],
      "metadata": {
        "id": "weq4rPxdsChT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These models and techniques cover a wide range of machine learning tasks, from classification and regression to clustering and dimensionality reduction. They are well-documented and highly efficient, making scikit-learn a popular choice for machine learning practitioners."
      ],
      "metadata": {
        "id": "V1e-ItS8sGDY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smuCiAReqhVT"
      },
      "outputs": [],
      "source": []
    }
  ]
}